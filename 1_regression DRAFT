# 1 Regression with `CO2`

## Declarations

- [ ] I have gone through the CHECKLIST.qmd before submitting this exercise.

Check the following if true:

- [ ] I have used generative AI to create answers in this exercise.
- [ ] I have used generative AI as a sparring partner (or used tools such as Github Copilot).

## Prelude
The file `co2.csv` contains car specifications, fuel consumption, and CO2 emissions.

| Column | Description |
| ------ | ----------- |
| Brand | The brand or manufacturer of the vehicle (e.g., Toyota, Ford, BMW).
| Vehicle Type | Classification of vehicles based on size and usage (e.g., SUV, Sedan).
| Engine Size (L) | Engine displacement volume in liters.
| Cylinders | Number of cylinders in the engine.
| Transmission | Type of transmission (e.g., Automatic, Manual).
| Fuel Type | Type of fuel used by the vehicle (e.g., Gasoline, Diesel, Hybrid).
| Fuel Consumption (City, Hwy, and Combined) |  Fuel efficiency measured in liters per 100 kilometers (L/100 km).
| CO2 Emissions (g/km) |  Carbon dioxide emissions per kilometer (**target**).

## 3.1 Import and investigate
::: {.callout-note icon=false appearance="simple"} 
1. Import and investigate the data. Should one of the columns be an index?

```{python}
pip install statsmodels
```

```{python}
#| code-fold: True
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as smf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, StackingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV
```

```{python}
co2 = pd.read_csv("co2.csv")
df = co2.copy()
```

```{python}
df.head()
df.info()
```


```{python}
# Checking duplicate values 
duplicate = df[df.duplicated(keep=False)]
duplicate  

## Some rows are completely identical, while others have small variations 
```
The columns cannot be an index as none of the columns include values that are unique to each row, and re-occur throughout the dataset (i,e, Brand, which signifies the manufacturer of the vehicle, would appear multiple times and so would the model and vehicle class).

Moreover, the columns such as cylinder, transmission, fuel type etc.. also cannot be included as an index column. 

```{python}
#check number of unique values
def get_unique_values(df):
    
    output_data = []

    for col in df.columns:

        # If the number of unique values in the column is less than or equal to 5
        if df.loc[:, col].nunique() <= 10:
            # Get the unique values in the column
            unique_values = df.loc[:, col].unique()
            # Append the column name, number of unique values, unique values, and data type to the output data
            output_data.append([col, df.loc[:, col].nunique(), unique_values, df.loc[:, col].dtype])
        else:
            # Otherwise, append only the column name, number of unique values, and data type to the output data
            output_data.append([col, df.loc[:, col].nunique(),"-", df.loc[:, col].dtype])

    output_df = pd.DataFrame(output_data, columns=['Column Name', 'Number of Unique Values', ' Unique Values ', 'Data Type'])

    return output_df

get_unique_values(df)

```


2. How many columns and rows are there?

```{python}
rows, columns = df.shape 

print(f"Rows:{rows}")
print(f"Columns:{columns}")
```

3. Which variables are numeric and which are categorical?

We begin by checking the data types included within the dataset. 
```{python}
print(df.dtypes)
```

```{python}
def get_unique_values(df):
    output_data = []

    for col in df.columns:
        # Check if the column is numerical or categorical
        col_type = 'Numerical' if pd.api.types.is_numeric_dtype(df[col]) else 'Categorical'
        
        # If the column is categorical or has 10 or fewer unique values, display the unique values
        if df.loc[:, col].nunique() <= 10 or col_type == 'Categorical':
            unique_values = df.loc[:, col].unique()
            output_data.append([col, df.loc[:, col].nunique(), unique_values, col_type])
        else:
            # For numerical columns with more than 10 unique values, omit listing unique values
            output_data.append([col, df.loc[:, col].nunique(), "-", col_type])

    # Create the output DataFrame
    output_df = pd.DataFrame(output_data, columns=['Column Name', 'Number of Unique Values', 'Unique Values', 'Type'])

    return output_df

get_unique_values(df)
```

The columns Make, Model, Vehicle Class, Transmission and Fuel Type are categorical.

The columns Engine Size (L), Cylinders, Fuel Consumption City(L/100km), Fuel Consumption Hwy(L/100km), Fuel Consumption Comb (L/100), Fuel Consumption Comb (mpg) and CO2 Emissions (g/km) are numerical variables. 

4. Are there any `N/A`s in the data?
:::

```{python}
# Checking missing values 
missing_count = df.isnull().sum()
value_count = df.isnull().count()
missing_percentage = round(missing_count / value_count * 100, 2)
missing_df = pd.DataFrame({"count": missing_count, "percentage": missing_percentage})
missing_df
```

Conclusion: No missing values; no 'N/A' values 

## 3.2 Correlations
::: {.callout-note icon=false appearance="simple"} 
Investigate the correlation matrix between `CO2 Emissions(g/km)` and the other (numerical) variables. Do *not* show the whole correlation matrix, just the column corresponding to `CO2 Emissions(g/km)`. Provide some informative comments.
:::

```{python}
# | code-summary: Correlation matrix. 
#| code-fold: True
correlation_matrix = df.select_dtypes(include='number').corr()
co2_correlation = correlation_matrix["CO2 Emissions(g/km)"]
co2_correlation
```
All of the numerical variables except for 'Fuel Consumption Comb(mpg)' have strong correlation with CO2 emissions, with Fuel consumption in the city and the combined Fuel consumption in the city and highway have the strongest correlation. 

The weak correlation between fuel consumption (mpg) and CO2 emissions (g/km) could be due to differences in units, as mpg is inversely proportional and measured in imperial units, while CO2 emissions are directly proportional and measured in metric units.

```

## 3.3 Bar graphs
::: {.callout-note icon=false appearance="simple"} 
Make some informative plots (e.g., bar graphs) relating the target variable to the relevant non-numeric features of `CO2`. Make the plots readable, and be sure to comment each plot.
:::
This plot shows fuel types and how much they contribute to CO2 emissions. Overall, numbers are pretty high. However, Fuel Type E and Z make up the highest CO2 emissions. 

```{python}

```
```{python}
# | code-summary: Fuel Type vs. Emissions
#| code-fold: True
plt.figure(figsize=(10, 6))
sns.barplot(x='Fuel Type', y='CO2 Emissions(g/km)', data=df, palette='coolwarm')
plt.title("CO2 Emissions by Fuel Type")
plt.xlabel("Fuel Type")
plt.ylabel("CO2 Emissions (g/km)")
plt.show()
```


This bar plot shows CO2 emissions emitted per each vehicle class. As seen from the graph, number 1 contributor is Van - Passenger and the least harmful vehicle class type is Station Wagon - small.
```{python}
# | code-summary: Vehicle Class vs. Emissions.
#| code-fold: True
plt.figure(figsize=(10, 6))
sns.barplot(x='CO2 Emissions(g/km)', y='Vehicle Class', data=df, palette='muted')
plt.title("CO2 Emissions by Vehicle Class")
plt.ylabel("Vehicle Class")
plt.xlabel("CO2 Emissions (g/km)")
plt.tight_layout()
plt.show()
```

This bar plot shows relationship between Transmission and CO2 emissions. Given transmission types determine how the engine power is delivered to the wheels. In this case, AV10 and AV7 are the most CO2 emittors. 
```{python}
# | code-summary: Transmission vs. Emissions
#| code-fold: True
plt.figure(figsize=(10, 6))
sns.barplot(x='Transmission', y='CO2 Emissions(g/km)', data=df, palette='muted')
plt.title("CO2 Emissions by Transmission")
plt.xlabel("Transmission")
plt.ylabel("CO2 Emissions (g/km)")
plt.tight_layout()
plt.show()
```



## 3.4 More correlations
::: {.callout-note icon=false appearance="simple"} 
Investigate the whole correlation matrix, but don't report it. Are there any interesting correlations between the features worth noting? 

Inspect a pair plot to check for linearity. Report 1 - 4 graphs illustrating your most important findings.
:::
```{python}
#|echo : False
df.corr(numeric_only=True).style.background_gradient(
    cmap="coolwarm", axis=None, vmin=-1, vmax=1
).format(precision=1)
```

```{python}
correlation_matrix = df.select_dtypes(include='number').corr()
correlation_matrix
```
Well, there a few things worth noting about the whole correlation matrix. 
1. Fuel Consumptions measured in (L/100 km) have strong correlation with each other. 
2. Fuel Consumption Comb(mpg) is the only variable that is least correlated with other features. That could be because of difference in units for sure. 
3. Overall, correlation coefficients are above 
0.7 which shows strong connection. 

```{python}
# | code-summary: Fuel Consumption(mpg) vs. Emissions
# | code-fold: true
sns.pairplot(df[["CO2 Emissions(g/km)", "Fuel Consumption Comb (mpg)"]])
```

The plot above displays a clear negative relationship between Fuel Consumption Comb (mpg) and CO2 Emissions; a higher mpg (combined fuel consumption) would mean an improved fuel efficiency and lead to lower emissions. 

There are some outliers, particularly in vehicles with higher emissions but relatively low mpg. These outliers can be due to the presence of  larger and/or less efficient vehicle models. 

There are no obvious clusters in the plot, which suggest a linear relationship beting present without the presence of distinct groups within the data. 

```{python}
# | code-summary: Fuel Consumptions in (L/100 km) vs. Fuel Consumption(mpg) 
# | code-fold: true
sns.pairplot(df[[ "Fuel Consumption City (L/100 km)", "Fuel Consumption Hwy (L/100 km)", "Fuel Consumption Comb (L/100 km)", "Fuel Consumption Comb (mpg)"]])
```

This plot above displays a strong negative correlation between Fuel Consumption (L/100 km) and Fuel Consumption Comb (mpg), which shows the inverse relationship that is formed because of the difference in units, where mpg measures the efficiency in imperial units whereas km is the metric equivalent. 

There are no significant outliers that are visible, however, some vehicles have extreme values, which can be explained due to the presence of larger vehicles in the dataset with higher consumption. 

A slight seperation between Fuel Consumption City (L/100 km) and and Fuel Consumption Hwy (L/100 km) is present, suggesting that city consumption is usually higher and less fuel-efficient. 

```{python}
# | code-summary: Engine Size vs. CO₂ Emissions
# | code-fold: true
sns.pairplot(df[["Engine Size(L)", "CO2 Emissions(g/km)"]])
```

The plot above shows a positive relationship between Engine Size and CO2 Emissions; larger engine sezes tend to produce higher emissions due to the increased fuel consumption required. 

There are some outliers present with larger engines (e.g. above 7), that could be due to other factors such as the engine or fuel type suggesting that engine size alone does not always perfectly correlate with emissions. 

There is no strong clustering of points, but a pattern shows that larger engine sizes tend to cluster around higher CO2 Emissions. 

```{python}
# | code-summary: All Fuel Consumption Metrics vs. CO₂ Emissions
# | code-fold: true
sns.pairplot(df[["Fuel Consumption City (L/100 km)", 
                 "Fuel Consumption Hwy (L/100 km)", 
                 "Fuel Consumption Comb (L/100 km)", 
                 "Fuel Consumption Comb (mpg)", 
                 "CO2 Emissions(g/km)"]])
```

The plot above shows a positive correlation between Fuel consumption (city, highway and combined) in metrics and CO2 Emissions; a higher fuel consumption leads to higher emissions. The mpg variable, however, has a negative relationship with CO2 emissions; a higher mpg (implying improved fuel efficiency) leads to lower emissions.

Outliers are more visible for Fuel Consumption (L/100 km), particularly where vehicles have high fuel consumption but relatively low emissions. This may indicate inefficiencies such as larger engines, non-optimal driving conditions, or less efficient vehicle types.

There are small clusters where fuel consumption values for City and Combined are highly correlated, showing that city driving tends to be more inefficient than highway driving (more constant speeds). mpg and L/100 km metrics are generally inversely clustered.

## 3.5 Linear regression
::: {.callout-note icon=false appearance="simple"} 
Using your insights from the previous exercise, fit a linear regression model. You should aim for an adjusted $R^2$ above `.99`.
:::

```{python}
# | echo: false
categorical_cols = ['Model', 'Vehicle Class', 'Transmission', 'Fuel Type']
```

```{python}
# | echo: false
df.rename(columns={'Engine Size(L)': 'engine_size',
'Fuel Consumption City (L/100 km)': 'fuel_consumption_city',
'Fuel Consumption Hwy (L/100 km)':'fuel_consumption_hwy',
'Fuel Consumption Comb (L/100 km)': 'fuel_consumption_comb_l',
'Fuel Consumption Comb (mpg)': 'fuel_consumption_comb_mpg',
'CO2 Emissions(g/km)': 'co2_emissions'}, inplace=True)
```

```{python}
df.info()
```
```{python}
#| echo: False
covariates = df.columns.drop(categorical_cols+['co2_emissions'])
fits = {cov: smf.ols("co2_emissions ~ " + cov, data = df).fit() for cov in covariates}
```

```{python}
# | echo: false
df.rename(columns={'Engine Size(L)': 'engine_size',
'Fuel Consumption City (L/100 km)': 'fuel_consumption_city',
'Fuel Consumption Hwy (L/100 km)':'fuel_consumption_hwy',
'Fuel Consumption Comb (L/100 km)': 'fuel_consumption_comb_l',
'Fuel Consumption Comb (mpg)': 'fuel_consumption_comb_mpg',
'CO2 Emissions(g/km)': 'co2_emissions',
'Fuel Type': 'fuel_type'}, inplace=True)
```

```{python}
import statsmodels.formula.api as smf
model = smf.ols("co2_emissions ~ engine_size + fuel_consumption_comb_l", data=df).fit()
print(model.rsquared)
print(model.rsquared_adj)
```
Why are we removing Cylinders, Fuel consumption City, Highway and mpg?

1. Engine Size (L) and Cylinders are highly correlated (r=0.9). Including both introduces redundancy and multicollinearity.
2. "Fuel Consumption City," "Highway," and "Combined" are extremely correlated (r>0.9). Keeping all three would create issues because they are almost identical predictors.
Balanced representation: "Combined (L/100 km)" is a weighted average of City and Highway fuel consumption, making it the most representative measure of a vehicle’s overall fuel consumption.
3. Inverse Relationship: Fuel Consumption (mpg) has a strong negative correlation (r=−0.9) with all the L/100 km metrics.Choose one system (either mpg or L/100 km) to avoid confusion. Since L/100 km is already included, mpg can be dropped.
4. Adding "Cylinders," "City," or "Highway" adds no new information

## 3.6 *p*-values
::: {.callout-note icon=false appearance="simple"} 
1. Provide the parameter estimates and *p*-values of your chosen model in a data frame. 
2. Roughly explain what the *p*-values are and what they can be used for.
3. Discuss the *p*-values in the table. How could you, potentially, use them to improve your model? (Only explain, you don't need to fit another model.)
:::

```{python}
pd.DataFrame({"Coefficient": model.params, "p-value": model.pvalues})
```

```{python}

import statsmodels.formula.api as smf
model1 = smf.ols("co2_emissions ~ engine_size + fuel_consumption_comb_l + fuel_consumption_city + fuel_consumption_hwy ", data=df).fit()
print(model1.rsquared)
print(model1.rsquared_adj)
```

```{python}
pd.DataFrame({"Coefficient": model1.params, "p-value": model1.pvalues})
```

In the second model, that is when everything was included, we see that pvalues are large. That means those variables are insignifant. When we include insignifact features, it creates redundancy and increase adjusted r^2 only by very small increase. This tiny improvement is not worth the added complexity, especially since the new variables are not significant (high p-values).

## 3.7 Another model
::: {.callout-note icon=false appearance="simple"} 
Suppose you only had the features `Fuel Type` and `Fuel Consumption Comb (mpg)` available. Fit an excellent model to this data.
:::

```{python}
df.columns
```
Model-with-interaction-form: 
(fuel_consumption_comb_mpg * C(fuel_type)): This term models the interaction between fuel_consumption_comb_mpg and fuel_type. This allows the effect of fuel consumption on co2_emissions to vary by fuel type.

```{python}
model = smf.ols("co2_emissions ~ fuel_consumption_comb_mpg*C(fuel_type)", data=df).fit()

print(model.rsquared_adj)
```
Model-with-no-interaction:
```{python}
model_no_interaction = smf.ols("co2_emissions ~ fuel_consumption_comb_mpg+C(fuel_type)", data=df).fit()
print(model_no_interaction.rsquared_adj)
```
First model provides us with better Adjusted R-squared. HOwever, both warn us with multicollinearity issues. Let's check the correlation matrix. 

```{python}
df_encoded = pd.get_dummies(df, columns=['fuel_type'], drop_first=True)
```

```{python}
correlation_matrix = df_encoded[['fuel_consumption_comb_mpg', 'fuel_type_E', 'fuel_type_N', 'fuel_type_Z', 'fuel_type_X']].corr()

plt.figure(figsize=(8, 6))  
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True, square=True)

plt.title("Correlation Matrix Heatmap", fontsize=16)
plt.show()
```

This correlation matrix between fuel_consumption_comb_mpg and every fuel type is not showing a lot of multicollinearity. So where is the warning coming from?


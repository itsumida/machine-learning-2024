# 1 Regression with `CO2`

## Declarations

- [X] I have gone through the CHECKLIST.qmd before submitting this exercise.

Check the following if true:

- [ ] I have used generative AI to create answers in this exercise.
- [X] I have used generative AI as a sparring partner (or used tools such as Github Copilot).

## Prelude
The file `co2.csv` contains car specifications, fuel consumption, and CO2 emissions.

| Column | Description |
| ------ | ----------- |
| Brand | The brand or manufacturer of the vehicle (e.g., Toyota, Ford, BMW).
| Vehicle Type | Classification of vehicles based on size and usage (e.g., SUV, Sedan).
| Engine Size (L) | Engine displacement volume in liters.
| Cylinders | Number of cylinders in the engine.
| Transmission | Type of transmission (e.g., Automatic, Manual).
| Fuel Type | Type of fuel used by the vehicle (e.g., Gasoline, Diesel, Hybrid).
| Fuel Consumption (City, Hwy, and Combined) |  Fuel efficiency measured in liters per 100 kilometers (L/100 km).
| CO2 Emissions (g/km) |  Carbon dioxide emissions per kilometer (**target**).

## 3.1 Import and investigate
::: {.callout-note icon=false appearance="simple"} 
1. Import and investigate the data. Should one of the columns be an index?

```{python}
# | code-summary: Importing packages 
#| code-fold: True
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as smf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, StackingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV 
import warnings 
warnings.filterwarnings('ignore')
```

```{python}
# | code-summary: First five rows of dataframe displayed
#| code-fold: True
co2 = pd.read_csv("co2.csv")
df = co2.copy()
df.head()
```

```{python}
df.info()
```

```{python}
print(df.isnull().sum())
```

```{python}
# | code-summary: Checking duplicate values
#| code-fold: True
duplicate = df[df.duplicated(keep=False)]
duplicate  

## Some rows are completely identical, while others have small variations. 
```

The columns cannot be an index as none of the columns include values that are unique to each row, and re-occur throughout the dataset (i,e, Brand, which signifies the manufacturer of the vehicle, would appear multiple times and so would the model and vehicle class).

Moreover, the columns such as cylinder, transmission, fuel type etc.. also cannot be included as an index column. 

If we were to create an index, it would preferrable to have a default integer index as when filtering or performing row-specific operations this would be more useful. 


```{python}
# | code-summary: Find the number of unique values 
#| code-fold: True
def get_unique_values(df):
    output_data = [
        [col, df[col].nunique(), df[col].unique() if df[col].nunique() <= 10 else "-", df[col].dtype]
        for col in df.columns
    ]
    return pd.DataFrame(output_data, columns=['Column Name', 'Number of Unique Values', 'Unique Values', 'Data Type'])

get_unique_values(df)
```

2. How many columns and rows are there?

```{python}
# | code-summary: Finding the number of columns and rows 
#| code-fold: True
rows, columns = df.shape 

print(f"Rows:{rows}")
print(f"Columns:{columns}")
```

3. Which variables are numeric and which are categorical?

We begin by checking the data types included within the dataset. 
```{python}
print(df.dtypes)
```

```{python}
# | code-summary: Displaying numerical and categorical variables 
#| code-fold: True
def get_unique_values(df):
    output_data = []
    
    for col in df.columns:
        # Determine if the column is Numerical or Categorical
        col_type = 'Numerical' if pd.api.types.is_numeric_dtype(df[col]) else 'Categorical'
        
        # Collect unique values if categorical or has 10 or fewer unique entries
        unique_values = df[col].unique() if df[col].nunique() <= 10 or col_type == 'Categorical' else "-"
        
        output_data.append([col, df[col].nunique(), unique_values, col_type])

    # Convert the collected data to a DataFrame
    return pd.DataFrame(output_data, columns=['Column Name', 'Number of Unique Values', 'Unique Values', 'Type'])

get_unique_values(df)
```

The columns Make, Model, Vehicle Class, Transmission and Fuel Type are categorical.

The columns Engine Size (L), Cylinders, Fuel Consumption City(L/100km), Fuel Consumption Hwy(L/100km), Fuel Consumption Comb (L/100), Fuel Consumption Comb (mpg) and CO2 Emissions (g/km) are numerical variables. 

4. Are there any `N/A`s in the data?
:::

```{python}
# | code-summary: Checking missing values
#| code-fold: True
missing_df = pd.DataFrame({
    'count': df.isnull().sum()
})

missing_df

```

Conclusion: No missing values; no 'N/A' values 

## 3.2 Correlations
::: {.callout-note icon=false appearance="simple"} 
Investigate the correlation matrix between `CO2 Emissions(g/km)` and the other (numerical) variables. Do *not* show the whole correlation matrix, just the column corresponding to `CO2 Emissions(g/km)`. Provide some informative comments.
:::

```{python}
# | code-summary: Correlation matrix. 
#| code-fold: True
correlation_matrix = df.select_dtypes(include='number').corr()
co2_correlation = correlation_matrix["CO2 Emissions(g/km)"]
co2_correlation
```
All of the numerical variables except for 'Fuel Consumption Comb(mpg)' have strong correlation with CO2 emissions, with Fuel consumption in the city and the combined Fuel consumption in the city and highway have the strongest correlation. 
The weak correlation between fuel consumption (mpg) and CO2 emissions (g/km) could be due to differences in units, as mpg is inversely proportional and measured in imperial units, while CO2 emissions are directly proportional and measured in metric units.
```

```{python}

```

## 3.3 Bar graphs
::: {.callout-note icon=false appearance="simple"}
Make some informative plots (e.g., bar graphs) relating the target variable to the relevant non-numeric features of `CO2`. Make the plots readable, and be sure to comment each plot.
:::

```{python}

```

```{python}
#| code-fold: True
#| code-summary: "Fuel Type vs. Emissions"
#| label: fig-fuel-type-emissions

plt.figure(figsize=(8, 5))
sns.barplot(x='Fuel Type', y='CO2 Emissions(g/km)', data=df, hue='Fuel Type', palette='coolwarm', legend=False)
plt.title("CO2 Emissions by Fuel Type")
plt.xlabel("Fuel Type")
plt.ylabel("CO2 Emissions (g/km)")
plt.show()
```

This plot (see @fig-fuel-type-emissions) shows fuel types and how much they contribute to CO2 emissions. Overall, the numbers are pretty high. However, Fuel Type E and Z make up the highest CO2 emissions.

```{python}
# | code-summary: Vehicle Class vs. Emissions.
#| code-fold: True
#| label: fig-vehicle-class-emissions

plt.figure(figsize=(7, 6))
sns.barplot(x='CO2 Emissions(g/km)', y='Vehicle Class', data=df, hue='Vehicle Class', palette='muted', legend=False)
plt.title("CO2 Emissions by Vehicle Class")
plt.ylabel("Vehicle Class")
plt.xlabel("CO2 Emissions (g/km)")
plt.tight_layout()
plt.show()
```

This plot (see @fig-vehicle-class-emissions) shows CO2 emissions emitted per each vehicle class. As seen from the graph, the number one contributor is Van - Passenger, and the least harmful vehicle class type is Station Wagon - small.

```{python}
# | code-summary: Transmission vs. Emissions
#| code-fold: True
#| label: fig-transmission-emissions

plt.figure(figsize=(9, 5))
sns.barplot(x='Transmission', y='CO2 Emissions(g/km)', data=df, hue='Transmission', palette='muted', legend=False)
plt.title("CO2 Emissions by Transmission")
plt.xlabel("Transmission")
plt.ylabel("CO2 Emissions (g/km)")
plt.tight_layout()
plt.show()
```

This plot (see @fig-transmission-emissions) shows the relationship between Transmission and CO2 emissions. Given that transmission types determine how the engine power is delivered to the wheels, it can be seen that AV10 and AV7 are the most significant CO2 emitters.

## 3.4 More correlations
::: {.callout-note icon=false appearance="simple"} 
Investigate the whole correlation matrix, but don't report it. Are there any interesting correlations between the features worth noting? 

Inspect a pair plot to check for linearity. Report 1 - 4 graphs illustrating your most important findings.
:::
```{python}
# | code-fold: true
#| output: false
df.corr(numeric_only=True).style.background_gradient(
    cmap="coolwarm", axis=None, vmin=-1, vmax=1
).format(precision=1)
```

```{python}
# | code-fold: true
#| output: false
correlation_matrix = df.select_dtypes(include='number').corr()
correlation_matrix
```
Well, there a few things worth noting about the whole correlation matrix. 

1. Fuel Consumptions measured in (L/100 km) have strong correlation with each other. 
2. Fuel Consumption Comb(mpg) is the only variable that is least correlated with other features. That could be because of difference in units for sure. 
3. Overall, correlation coefficients are above 
0.7 which shows strong connection. 

```{python}
# | code-summary: Fuel Consumption(mpg) vs. Emissions
# | code-fold: true
# | label: fig-fuel-consumption-emissions

sns.pairplot(df[["CO2 Emissions(g/km)", "Fuel Consumption Comb (mpg)"]])
```

The plot (see @fig-fuel-consumption-emissions) above displays a clear negative relationship between Fuel Consumption Comb (mpg) and CO2 Emissions. A higher mpg (combined fuel consumption) would mean improved fuel efficiency and lead to lower emissions.

There are some outliers, particularly in vehicles with higher emissions but relatively low mpg. These outliers could be due to the presence of larger and/or less efficient vehicle models.

There are no obvious clusters in the plot, which suggests a linear relationship is present without distinct groups within the data.

```{python}
# | code-summary: Fuel Consumptions in (L/100 km) vs. Fuel Consumption(mpg) 
# | code-fold: true
# | label: fig-fuel-consumption-comb-mpg

sns.pairplot(df[[ "Fuel Consumption City (L/100 km)", "Fuel Consumption Hwy (L/100 km)", "Fuel Consumption Comb (L/100 km)", "Fuel Consumption Comb (mpg)"]])
```

The plot (see @fig-fuel-consumption-comb-mpg) above displays a strong negative correlation between Fuel Consumption (L/100 km) and Fuel Consumption Comb (mpg), highlighting the inverse relationship caused by the difference in units—mpg measures efficiency in imperial units, while km is the metric equivalent.

Although no significant outliers are visible, some vehicles have extreme values, which can be attributed to the presence of larger vehicles in the dataset with higher consumption.

A slight separation between Fuel Consumption City (L/100 km) and Fuel Consumption Hwy (L/100 km) is present, suggesting that city consumption is usually higher and less fuel-efficient.

```{python}
# | code-summary: Engine Size vs. CO₂ Emissions
# | code-fold: true
# | label: fig-engine-size-emissions

sns.pairplot(df[["Engine Size(L)", "CO2 Emissions(g/km)"]])
```

The plot (see @fig-engine-size-emissions) above shows a positive relationship between Engine Size and CO2 Emissions; larger engine sizes tend to produce higher emissions due to the increased fuel consumption required.

There are some outliers present with larger engines (e.g., above 7), which could be due to other factors such as the engine or fuel type, suggesting that engine size alone does not always perfectly correlate with emissions.

While there is no strong clustering of points, a pattern shows that larger engine sizes tend to cluster around higher CO2 emissions.

```{python}
# | code-summary: All Fuel Consumption Metrics vs. CO₂ Emissions
# | code-fold: true
# | label: fig-fuel-consumption-all-emissions

sns.pairplot(df[["Fuel Consumption City (L/100 km)", 
                 "Fuel Consumption Hwy (L/100 km)", 
                 "Fuel Consumption Comb (L/100 km)", 
                 "Fuel Consumption Comb (mpg)", 
                 "CO2 Emissions(g/km)"]])
```

The plot (see @fig-fuel-consumption-all-emissions) above shows a positive correlation between fuel consumption (city, highway, and combined) in metrics and CO2 emissions; higher fuel consumption leads to higher emissions. The mpg variable, however, has a negative relationship with CO2 emissions; a higher mpg (implying improved fuel efficiency) leads to lower emissions.

Outliers are more visible for Fuel Consumption (L/100 km), particularly where vehicles have high fuel consumption but relatively low emissions. This may indicate inefficiencies such as larger engines, non-optimal driving conditions, or less efficient vehicle types.

There are small clusters where fuel consumption values for City and Combined are highly correlated, showing that city driving tends to be more inefficient than highway driving (which generally involves more constant speeds). The mpg and L/100 km metrics are generally inversely clustered.

## 3.5 Linear regression
::: {.callout-note icon=false appearance="simple"} 
Using your insights from the previous exercise, fit a linear regression model. You should aim for an adjusted $R^2$ above `.99`.
:::

```{python}
# | echo: false
categorical_cols = ['Model', 'Vehicle Class', 'Transmission', 'Fuel Type']
```

```{python}
# | echo: false
df.rename(columns={'Engine Size(L)': 'engine_size',
'Fuel Consumption City (L/100 km)': 'fuel_consumption_city',
'Fuel Consumption Hwy (L/100 km)':'fuel_consumption_hwy',
'Fuel Consumption Comb (L/100 km)': 'fuel_consumption_comb_l',
'Fuel Consumption Comb (mpg)': 'fuel_consumption_comb_mpg',
'CO2 Emissions(g/km)': 'co2_emissions'}, inplace=True)
```

```{python}
df.info()
```
```{python}
#| echo: False
covariates = df.columns.drop(categorical_cols+['co2_emissions'])
fits = {cov: smf.ols("co2_emissions ~ " + cov, data = df).fit() for cov in covariates}
```

```{python}
# | echo: false
df.rename(columns={'Engine Size(L)': 'engine_size',
'Fuel Consumption City (L/100 km)': 'fuel_consumption_city',
'Fuel Consumption Hwy (L/100 km)':'fuel_consumption_hwy',
'Fuel Consumption Comb (L/100 km)': 'fuel_consumption_comb_l',
'Fuel Consumption Comb (mpg)': 'fuel_consumption_comb_mpg',
'CO2 Emissions(g/km)': 'co2_emissions',
'Fuel Type': 'fuel_type'}, inplace=True)
```

```{python}
import statsmodels.formula.api as smf
model = smf.ols("co2_emissions ~ engine_size + fuel_consumption_comb_l", data=df).fit()
print(model.rsquared)
print(model.rsquared_adj)
```
Why are we removing Cylinders, Fuel consumption City, Highway and mpg?

1. Engine Size (L) and Cylinders are highly correlated (r=0.9). Including both introduces redundancy and multicollinearity.
2. "Fuel Consumption City," "Highway," and "Combined" are extremely correlated (r>0.9). Keeping all three would create issues because they are almost identical predictors.
Balanced representation: "Combined (L/100 km)" is a weighted average of City and Highway fuel consumption, making it the most representative measure of a vehicle’s overall fuel consumption.
3. Inverse Relationship: Fuel Consumption (mpg) has a strong negative correlation (r=−0.9) with all the L/100 km metrics.Choose one system (either mpg or L/100 km) to avoid confusion. Since L/100 km is already included, mpg can be dropped.
4. Adding "Cylinders," "City," or "Highway" adds no new information

## 3.6 *p*-values
::: {.callout-note icon=false appearance="simple"} 
1. Provide the parameter estimates and *p*-values of your chosen model in a data frame. 
2. Roughly explain what the *p*-values are and what they can be used for.
3. Discuss the *p*-values in the table. How could you, potentially, use them to improve your model? (Only explain, you don't need to fit another model.)
:::

```{python}
pd.DataFrame({"Coefficient": model.params, "p-value": model.pvalues})
```

```{python}
import statsmodels.formula.api as smf
model1 = smf.ols("co2_emissions ~ engine_size + fuel_consumption_comb_l + fuel_consumption_city + fuel_consumption_hwy ", data=df).fit()
print(model1.rsquared)
print(model1.rsquared_adj)
```

```{python}
pd.DataFrame({"Coefficient": model1.params, "p-value": model1.pvalues})
```

In the second model, that is when everything was included, we see that pvalues are large. That means those variables are insignifant. When we include insignifact features, it creates redundancy and increase adjusted r^2 only by very small increase. This tiny improvement is not worth the added complexity, especially since the new variables are not significant (high p-values).

p-values indicate the probability of observing the estimated effect if the predictor has no real impact (null hypothesis is true). A low p-value (typically < 0.05) suggests the predictor is significant, while a high p-value indicates the predictor is likely insignificant and may not meaningfully contribute to the model.

The p-values show that some predictors, like fuel_consumption_city and fuel_consumption_hwy, are not statistically significant (high p-values).

 Although we could remove these variables to simplify the model, multicollinearity with other predictors (like fuel_consumption_comb_l) might be causing the insignificance. Therefore, it's not strictly necessary to remove them if they provide useful context or if multicollinearity is unavoidable.

## 3.7 Another model
::: {.callout-note icon=false appearance="simple"} 
Suppose you only had the features `Fuel Type` and `Fuel Consumption Comb (mpg)` available. Fit an excellent model to this data.
:::

```{python}
df.columns
```
Model-with-interaction-form: 
(fuel_consumption_comb_mpg * C(fuel_type)): This term models the interaction between fuel_consumption_comb_mpg and fuel_type. This allows the effect of fuel consumption on co2_emissions to vary by fuel type.

```{python}
model = smf.ols("co2_emissions ~ fuel_consumption_comb_mpg*C(fuel_type)", data=df).fit()
print(model.rsquared_adj)
```
Model-with-no-interaction:
```{python}
model_no_interaction = smf.ols("co2_emissions ~ fuel_consumption_comb_mpg+C(fuel_type)", data=df).fit()
print(model_no_interaction.rsquared_adj)
```
First model provides us with better Adjusted R-squared. 

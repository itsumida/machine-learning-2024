# 2 Classification of air quality

## Declarations
- [ ] I have gone through the CHECKLIST.qmd before submitting this exercise.

Check the following if true:

- [ ] I have used generative AI to create answers in this exercise.
- [ ] I have used generative AI as a sparring partner (or used tools such as Github Copilot).

## Prelude

The data set `pollution.csv` contains the following variables.

| Variable | Description |
| -------- | ----------- | 
| Temperature (°C)                    | Average temperature of the region. |
| Humidity (%)                        | Relative humidity recorded in the region. |
| PM2.5 Concentration (µg/m³)         | Fine particulate matter levels. |
| PM10 Concentration (µg/m³)          | Coarse particulate matter levels. |
| NO2 Concentration (ppb)             | Nitrogen dioxide levels. |
| SO2 Concentration (ppb)             | Sulfur dioxide levels. |
| CO Concentration (ppm)              | Carbon monoxide levels. |
| Proximity to Industrial Areas (km)  | Distance to the nearest industrial zone. |
| Population Density (people/km²)     | Number of people per square kilometer in the region. |
| Air Quality Levels                  | Quantification of air quality **Target**. |

```{python}
# | echo: False
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
```
## 2.1 Import and investigate
::: {.callout-note icon=false appearance="simple"} 
## 
1. Import the data set `pollution.csv`. 
2. Remove every row containing missing values.
3. Make a feature matrix `X` and target vector `y`.

```{python}
import pandas as pd

data = pd.read_csv('pollution.csv')

data_cleaned = data.dropna()

X = data_cleaned.drop(columns=['Air Quality'])
y = data_cleaned['Air Quality']

print("Feature Matrix (X):")
print(X.head())

print("\nTarget Vector (y):")
print(y.head())

```
## 2.2 Inspect 
::: {.callout-note icon=false appearance="simple"} 
## 
1. Inspect the categories in the target using a suitable plotting device. 
   Is the target the imbalanced?  (I.e., the majority classes are far more prevalent than minority classes)
2. Inspect the pairplot. Display and explain any intersting patterns. 
:::

```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

file_path = 'pollution.csv'  
data = pd.read_csv(file_path)

data_cleaned = data.dropna()
target_counts = data_cleaned['Air Quality'].value_counts()

plt.figure(figsize=(8, 5))
sns.barplot(x=target_counts.index, y=target_counts.values, palette="viridis")
plt.title("Air Quality Category Distribution")
plt.xlabel("Air Quality Category")
plt.ylabel("Frequency")
plt.show()

print("Distribution of Air Quality Categories:")
print(target_counts)
print("\nClass Imbalance Ratio (Max/Min):", target_counts.max() / target_counts.min())

sns.pairplot(data_cleaned, hue='Air Quality', diag_kind='kde', corner=True, palette="coolwarm")
plt.suptitle("Pairplot of Features by Air Quality", y=1.02)
plt.show()

```

## 2.3 Logistic regression
::: {.callout-note icon=false appearance="simple"} 
## 
Remove the column `PP10` from the `pollution` dataset (and `X` and `y`). We will work with this modified data set in the remainder of the exercise.

Using cross-validation, evaluate the fit of an non-penalized logistic regression on the whole data set using (a) the negative log loss, (b) accuracy. 

What are the benefits and downsides of using the log loss vs. the accuracy?

(*Hint*: Use the argument `scoring = "neg_log_loss"`. Use the argument `warning: False` to suppress warnings in the output of a code block.)

```{python}
import pandas as pd

file_path = 'pollution.csv'  
data = pd.read_csv(file_path)

data_cleaned = data.dropna()


X = data_cleaned.drop(columns=['Air Quality'])
y = data_cleaned['Air Quality']


print("Feature Matrix (X):")
print(X.head())

print("\nTarget Vector (y):")
print(y.head())

(snswer the benefits and downside)

Log loss benefits:
1. Handles Imbalanced Data:
Log loss is less affected by class imbalances compared to accuracy. Since it considers the predicted probabilities for all classes, it gives a more nuanced evaluation of model performance.

2. More granular 
It provides a more detailed assessment of the model’s performance. Even small improvements in probability calibration are reflected in log loss, which makes it useful for fine-tuning models.

Log log loss downsides:
1. It is less intuitive than accuracy because it requires understanding probabilities and logarithmic penalties.

2. Log loss penalizes confident incorrect predictions very harshly, which can sometimes be overly sensitive

Accuracy benefits 
1.Simple and Intuitive:
Accuracy is easy to calculate and understand, making it a popular metric for quick evaluations.

2.Direct Measure of Performance:
It directly reflects the percentage of predictions that the model got right, which makes it straightforward to interpret.

Accuracy downsides
1. Imbalanced data.
When one or more class labels are extremely common.
Almost always the case. Think about churn prediction. Most customers don’t churn

2. Prediction certainty.
Accuracy does not take prediction certainty into account. A model predicing correctly with 51% probability can get the same score as a model with 99% probability.
Ranking outcomes is common – call the lead with the highest probability of buying first

```

## 2.4 Quadratic logistic regression
::: {.callout-note icon=false appearance="simple"} 
## 
It's natural to check for "simple" non-linearity by running quadratic logistic regressions.

1. Run a quadratic logistic regression and report its cross-validated log loss and accuracy. You have to use pipelines here.
2. Then run a quadratic logistic regression model using only interaction terms. 

Place the accuracy and log scores for these two models, and the model you made in the previous exercise, in a nicely formatted table. Which model performs best? What is happening here?



``
:::

```{python}

import pandas as pd
from sklearn.model_selection import cross_validate
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import PolynomialFeatures

file_path = 'pollution.csv'  
data = pd.read_csv(file_path)
data_cleaned = data.dropna()

X = data_cleaned.drop(columns=['Air Quality', 'PM10'])
y = data_cleaned['Air Quality']

print("Feature Matrix (X):")
print(X.head())

print("\nTarget Vector (y):")
print(y.head())

baseline_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('logistic', LogisticRegression(penalty=None, max_iter=1000))  
])

full_quadratic_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ('logistic', LogisticRegression(penalty=None, max_iter=1000))
])


interaction_only_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('poly', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),
    ('logistic', LogisticRegression(penalty=None, max_iter=1000))
])


scoring_metrics = ['neg_log_loss', 'accuracy']

baseline_cv_results = cross_validate(baseline_pipeline, X, y, cv=5, scoring=scoring_metrics)
full_quadratic_cv_results = cross_validate(full_quadratic_pipeline, X, y, cv=5, scoring=scoring_metrics)
interaction_only_cv_results = cross_validate(interaction_only_pipeline, X, y, cv=5, scoring=scoring_metrics)


print("\nBaseline Pipeline CV Results:")
print("Negative Log Loss (Mean):", -baseline_cv_results['test_neg_log_loss'].mean())
print("Accuracy (Mean):", baseline_cv_results['test_accuracy'].mean())

print("\nFull Quadratic Pipeline CV Results:")
print("Negative Log Loss (Mean):", -full_quadratic_cv_results['test_neg_log_loss'].mean())
print("Accuracy (Mean):", full_quadratic_cv_results['test_accuracy'].mean())

print("\nInteraction-Only Pipeline CV Results:")
print("Negative Log Loss (Mean):", -interaction_only_cv_results['test_neg_log_loss'].mean())
print("Accuracy (Mean):", interaction_only_cv_results['test_accuracy'].mean())

:::

```{python}

df = pd.read_csv('co2.csv', index_col='Make')

```

```{python}
# | code-summary: Summary of the dataset and counting null values.
#| code-fold: True
print(df.info())
print(df.isnull().sum())
```

Column Make can be chosen as an index. There are 12 columns(including index column) and 7385 rows. 

Numerical variables are Cylinders, Engine Size(L), Fuel Consumption City/Hwy/Comb/Comb(mpg) and CO2 emissions. 

Categorical values are Make, Model, Vehicle Class, Transmission, Fuel Type. 

There are no N/A values in the dataset. 


## 3.2 Correlations
::: {.callout-note icon=false appearance="simple"} 
Investigate the correlation matrix between `CO2 Emissions(g/km)` and the other (numerical) variables. Do *not* show the whole correlation matrix, just the column corresponding to `CO2 Emissions(g/km)`. Provide some informative comments.
:::

```{python}
# | code-summary: Correlation matrix. 
#| code-fold: True
correlation_matrix = df.select_dtypes(include='number').corr()
co2_correlation = correlation_matrix["CO2 Emissions(g/km)"]
co2_correlation
```
All of the numerical variables except for 'Fuel Consumption Comb(mpg)' have strong correlation with CO2 emissions. 

The weak correlation between fuel consumption (mpg) and CO2 emissions (g/km) could be due to differences in units, as mpg is inversely proportional and measured in imperial units, while CO2 emissions are directly proportional and measured in metric units. 

## 3.3 Bar graphs
::: {.callout-note icon=false appearance="simple"} 
Make some informative plots (e.g., bar graphs) relating the target variable to the relevant non-numeric features of `CO2`. Make the plots readable, and be sure to comment each plot.
:::

This plot shows fuel types and how much they contribute to CO2 emissions. Overall, numbers are pretty high. However, Fuel Type E and Z make up the highest CO2 emissions. 
```{python}
# | code-summary: Fuel Type vs. Emissions
#| code-fold: True
plt.figure(figsize=(10, 6))
sns.barplot(x='Fuel Type', y='CO2 Emissions(g/km)', data=df, palette='coolwarm')
plt.title("CO2 Emissions by Fuel Type")
plt.xlabel("Fuel Type")
plt.ylabel("CO2 Emissions (g/km)")
plt.show()
```

This bar plot shows CO2 emissions emitted per each vehicle class. As seen from the graph, number 1 contributor is Van - Passenger and the least harmful vehicle class type is Station Wagon - small.
```{python}
# | code-summary: Vehicle Class vs. Emissions.
#| code-fold: True
plt.figure(figsize=(10, 6))
sns.barplot(x='CO2 Emissions(g/km)', y='Vehicle Class', data=df, palette='muted')
plt.title("CO2 Emissions by Vehicle Class")
plt.ylabel("Vehicle Class")
plt.xlabel("CO2 Emissions (g/km)")
plt.tight_layout()
plt.show()
```

This bar plot shows relationship between Transmission and CO2 emissions. Given transmission types determine how the engine power is delivered to the wheels. In this case, AV10 and AV7 are the most CO2 emittors. 
```{python}
# | code-summary: Transmission vs. Emissions
#| code-fold: True
plt.figure(figsize=(10, 6))
sns.barplot(x='Transmission', y='CO2 Emissions(g/km)', data=df, palette='muted')
plt.title("CO2 Emissions by Transmission")
plt.xlabel("Transmission")
plt.ylabel("CO2 Emissions (g/km)")
plt.tight_layout()
plt.show()
```


## 3.4 More correlations
::: {.callout-note icon=false appearance="simple"} 
Investigate the whole correlation matrix, but don't report it. Are there any interesting correlations between the features worth noting? 

Inspect a pair plot to check for linearity. Report 1 - 4 graphs illustrating your most important findings.
:::

```{python}

#|echo : False
df.corr(numeric_only=True).style.background_gradient(
    cmap="coolwarm", axis=None, vmin=-1, vmax=1
).format(precision=1)
```

```{python}
correlation_matrix = df.select_dtypes(include='number').corr()
correlation_matrix
```
Well, there a few things worth noting about the whole correlation matrix. 
1. Fuel Consumptions measured in (L/100 km) have strong correlation with each other. 
2. Fuel Consumption Comb(mpg) is the only variable that is least correlated with other features. That could be because of difference in units for sure. 
3. Overall, correlation coefficients are above 
0.7 which shows strong connection. 

```{python}
# | code-summary: Fuel Consumption(mpg) vs. Emissions
# | code-fold: true
sns.pairplot(df[["CO2 Emissions(g/km)", "Fuel Consumption Comb (mpg)"]])
```

```{python}
# | code-summary: Fuel Consumptions in (L/100 km) vs. Fuel Consumption(mpg) 
# | code-fold: true
sns.pairplot(df[[ "Fuel Consumption City (L/100 km)", "Fuel Consumption Hwy (L/100 km)", "Fuel Consumption Comb (L/100 km)", "Fuel Consumption Comb (mpg)"]])
```


## 3.5 Linear regression
::: {.callout-note icon=false appearance="simple"} 
Using your insights from the previous exercise, fit a linear regression model. You should aim for an adjusted $R^2$ above `.99`.
:::
```{python}
# | echo: false
df.rename(columns={'Engine Size(L)': 'engine_size',
'Fuel Consumption City (L/100 km)': 'fuel_consumption_city',
'Fuel Consumption Hwy (L/100 km)':'fuel_consumption_hwy',
'Fuel Consumption Comb (L/100 km)': 'fuel_consumption_comb_l',
'Fuel Consumption Comb (mpg)': 'fuel_consumption_comb_mpg',
'CO2 Emissions(g/km)': 'co2_emissions',
'Fuel Type': 'fuel_type'}, inplace=True)
```
```{python}
import statsmodels.formula.api as smf
model = smf.ols("co2_emissions ~ engine_size + fuel_consumption_comb_l", data=df).fit()
print(model.rsquared)
print(model.rsquared_adj)
```
Why are we removing Cylinders, Fuel consumption City, Highway and mpg?

1. Engine Size (L) and Cylinders are highly correlated (r=0.9). Including both introduces redundancy and multicollinearity.
2. "Fuel Consumption City," "Highway," and "Combined" are extremely correlated (r>0.9). Keeping all three would create issues because they are almost identical predictors.
Balanced representation: "Combined (L/100 km)" is a weighted average of City and Highway fuel consumption, making it the most representative measure of a vehicle’s overall fuel consumption.
3. Inverse Relationship: Fuel Consumption (mpg) has a strong negative correlation (r=−0.9) with all the L/100 km metrics.Choose one system (either mpg or L/100 km) to avoid confusion. Since L/100 km is already included, mpg can be dropped.
4. Adding "Cylinders," "City," or "Highway" adds no new information

## 3.6 *p*-values
::: {.callout-note icon=false appearance="simple"} 
1. Provide the parameter estimates and *p*-values of your chosen model in a data frame. 
2. Roughly explain what the *p*-values are and what they can be used for.
3. Discuss the *p*-values in the table. How could you, potentially, use them to improve your model? (Only explain, you don't need to fit another model.)
:::

```{python}
pd.DataFrame({"Coefficient": model.params, "p-value": model.pvalues})
```

```{python}

import statsmodels.formula.api as smf
model1 = smf.ols("co2_emissions ~ engine_size + fuel_consumption_comb_l + fuel_consumption_city + fuel_consumption_hwy ", data=df).fit()
print(model.rsquared)
print(model.rsquared_adj)
```

```{python}
pd.DataFrame({"Coefficient": model1.params, "p-value": model1.pvalues})
```

In the second model, that is when everything was included, we see that pvalues are large. That means those variables are insignifant. When we include insignifact features, it creates redundancy and increase adjusted r^2 only by very small increase. This tiny improvement is not worth the added complexity, especially since the new variables are not significant (high p-values).

## 3.7 Another model
::: {.callout-note icon=false appearance="simple"} 
Suppose you only had the features `Fuel Type` and `Fuel Consumption Comb (mpg)` available. Fit an excellent model to this data.
:::





